#include <iostream>
#include <algorithm>
#include <vector>
#define endl "\n"

typedef long long ll;
using namespace std;

ll A, B;
ll d[60];

ll solve(ll num) {
    ll ans = num & 1;
    for(int i = 55; i > 0; i--) {
        if (num & (1LL << i)) {
            ans += d[i - 1] + (num - (1LL << i) + 1);
            num -= (1LL << i);
        }
    }
    return ans;
}

int main() {
    ios::sync_with_stdio(0);
    cin.tie(0);
    cin >> A >> B;

    d[0] = 1;
    for(int i = 1; i <= 55; i++) {
        d[i] = d[i - 1] * 2 + (1LL << i);
    }

    cout << solve(B) - solve(A - 1);
}

// 아래와 같이 최상위 비트가 켜져있을 때, 아래의 1의 수를 누적합으로 구해둔다.
// 계산해야 하는 수가 들어왔을 때, 상위 비트부터 검사를 한다. 
// 만약 상위 비트가 1이라면 그 아래 수의 모든 1의 수를 구할 수 있게 된다. d[i - 1]
// num이 11010 이라면 10000 밑의 수는 d[4 - 1] 이고 
// 1010까지의 상위비트인 10000의 수를 더해야줘야 한다. num - 2 ^ i + 1로 구할 수 있다. 
// 이후, 10000을 빼주고 계속 구해준다. (1 << i)를 빼준다.
// 마지막의 1의 자리는 반복문으로 진행할 수 없다. 왜냐면 1의 자리는 0승이기에 d[-1]은 가능하지 않기 때문이다.
// 따라서 처음부터 1의 자리를 계산하고 시작한다 ans = num & 1;

// 1 01 - 1 -> 1 = d[0]

// 2 10 - 1
// 3 11 - 2 -> 1 * 2 + 2 = 4 d[1]

// 4 100 - 1
// 5 101 - 2
// 6 110 - 2
// 7 111 - 3 -> 3 * 2 + 4 = 12 d[2]

// 8 1000 - 1 
// 9 1001 - 2 
// 10 1010 - 2 
// 11 1011 - 3 
// 12 1100 - 2 
// 13 1101 - 3 
// 14 1110 - 3 
// 15 1111 - 4 -> 12 * 2 + 8 = 32 d[3]

// 16 10000 - 1 
// 17 10001 - 2 
// 18 10010 - 2 
// 19 10011 - 3 
// 20 10100 - 2 
// 21 10101 - 3 
// 22 10110 - 3 
// 23 10111 - 4 
// 24 11000 - 2 
// 25 11001 - 3 
// 26 11010 - 3 
// 27 11011 - 4 
// 28 11100 - 3 
// 29 11101 - 4 
// 30 11110 - 4 
// 31 11111 - 5 -> 32 * 2 + 16 d[4]

// 32 100000 - 1